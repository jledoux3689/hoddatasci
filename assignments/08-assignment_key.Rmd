---
title: "Assignment 8 Key"
output: html_document
---
## Libraries
```{r}
library(arm)
library(modelr)
library(AUC)
library(knitr)
library(tidyverse)
```

## Read in Data
```{r}
tr<-read_csv("training.csv")

test<-read_csv("test.csv")

```


## 1. Calculate the proportion of lemons in the training dataset using the `IsBadBuy` variable. 

```{r}
tr%>%summarize(mean(IsBadBuy))
```



## 2. Calculate the proportion of lemons by Make. 
```{r}
tr%>%
  group_by(Make)%>%
  summarize(mean_lemon=mean(IsBadBuy))%>%
  arrange(-mean_lemon)%>%
  kable()
```

## Data Wrangling: Model and Year are going to be highly predicitve, but I want to only take the 100 or so most common
```{r}

model_prop=.001

tr<-tr%>%mutate(new_model_lump=fct_lump(as_factor(Model),prop = model_prop))

test<-test%>%mutate(new_model_lump=fct_lump(as_factor(Model),prop=model_prop))
           
miss_levels<-levels(test$new_model_lump)[!(levels(test$new_model_lump)%in%levels(tr$new_model_lump))]

test<-test%>%mutate(new_model_lump=ifelse(as.character(new_model_lump)%in%miss_levels,
                                          "Other",
                                          as.character(new_model_lump)))%>%
  mutate(new_model_lump=as_factor(new_model_lump))



```


## 4. Now, predict the probability of being a lemon using a linear model (`lm(y~x`), with covariates of your choosing from the training dataset. 

```{r}
mod_lm<-lm(IsBadBuy~
             as.factor(VehYear)+
             as.factor(new_model_lump)+
             VehicleAge+
             Auction+
             WarrantyCost,
           data=tr);summary(mod_lm)
```



## 5. Make predictions from the linear model.

```{r}
tr<-tr%>%add_predictions(mod_lm)%>%
  mutate(pred_lm=ifelse(pred<0,0,pred))%>% ## Trim <0s
  mutate(pred_lm=ifelse(pred>1,1,pred)) ## Trim >1s
```

## 6. Calculate the AUC for the linear predictions from the ROC against the outcome for the training dataset. 

```{r}
lm_roc<-roc(predictions =   tr$pred_lm, labels = as.factor(tr$IsBadBuy))
auc(lm_roc)
```

## 7. Now, predict the probability of being a lemon using a logistic regression (`glm(y~x,family=binomial(link="logit")`)), again using covariates of your choosing.  

```{r}
run_logit=TRUE

if(run_logit==TRUE){
mod_logit<-glm(IsBadBuy~
             as.factor(VehYear)+
             as.factor(new_model_lump)+
             VehicleAge+
             Auction+
             WarrantyCost,
            family=binomial(link="logit"),
           data=tr); print(summary(mod_logit))

save(mod_logit,file="mod_logit.Robject")
}else{
  load("mod_logit.Robject")
  summary(mod_logit)
  }

```


## 8. Make predictions from the logit model. Make sure these are probabilities. 

```{r}
tr<-tr%>%add_predictions(mod_logit)%>%
  mutate(pred_logit=arm::invlogit(pred)) ## This transforms to probabilities
  
```

## 9. Calculate the AUC for the predictions from the ROC based on the logit model. 

```{r}
roc_logit<-roc(predictions = tr$pred_logit,labels=as.factor(tr$IsBadBuy))
logit_auc<-auc(roc_logit)
logit_gini<-(2*logit_auc)-1;logit_gini
```


## 10. (optional) submit your predictions from the testing dataset as a late submission to Kaggle and see how you do against real-wolrd competition. 

```{r}         
test<-test%>%add_predictions(mod_logit)%>%
  mutate(pred_logit=arm::invlogit(pred))%>%
  mutate(IsBadBuy=pred_logit)

submit<-test%>%select(RefId,IsBadBuy)
  

write_csv(submit,path="submit.csv")

```

